
## ğŸŒ² Tree-Based Models

Tree-based methods are a class of non-linear models that split the feature space into rectangular regions. Theyâ€™re intuitive, interpretable, and applicable to both regression and classification.

---

## ğŸ§  Core Concepts

### ğŸŒ¿ 1. Decision Trees

- **Regression Trees**: Predict a continuous outcome.
- **Classification Trees**: Predict a categorical outcome.

They partition data recursively and assign a constant prediction in each final (leaf) node.

### ğŸª“ 2. Tree Construction

- Trees are built via **recursive binary splitting**.
- Splits are chosen by minimizing a **cost function**:
  - **Regression**: Residual Sum of Squares (RSS)
  - **Classification**: Gini, Entropy, or Misclassification Rate

---

## âœ‚ï¸ Pruning

- Prevents **overfitting** by trimming back the full tree.
- **Cost complexity pruning** controls model complexity using a parameter \( \alpha \).

---

## ğŸ§ª Ensemble Methods

### ğŸŒ³ Bagging

- Trains many trees on **bootstrapped** data samples.
- Final prediction is the **average** (regression) or **majority vote** (classification).
- **Reduces variance**.

### ğŸŒ² Random Forests

- Like bagging, but each split only considers a **subset of predictors**.
- Improves accuracy by de-correlating trees.

### ğŸ”¥ Boosting

- Builds trees **sequentially**, each one correcting the errors of the previous.
- Controlled via:
  - **Learning rate**
  - **Number of trees**
  - **Tree depth**

### ğŸ“¦ BART (Bayesian Additive Regression Trees)

- Bayesian model averaging over ensembles of trees.
- Captures uncertainty and performs regularization automatically.

---

## ğŸ“˜ References

- *An Introduction to Statistical Learning*, Chapter 8
- [statlearning.com](https://www.statlearning.com)
